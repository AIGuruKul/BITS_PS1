# **Viva Learning Outcomes**

I was able to answer all the questions related to RAG and my codebase, however I got two LLM-related questions incorrectly. This indicated my need to brush up on my knowledge of LLM fundamentals.

Q1. Why are LLMs called **Large** Language Models?  
A1. It is because they are trained on billions of **parameters**. However, I thought it was so because they are trained on a large amount of data. Even Small Language Models (SLMs) can be trained on huge datasets, but they still arenâ€™t called LLMs because their parameter count is small.

Q2. If LLMs have already been trained on some data, do we still need to store the data or can we use them without the data?  
A2. During training, the model learns patterns and knowledge from the data and stores that information in its parameters. After training however, the original data is no longer required for the model to function. We can use the model using the saved parameters.   
I incorrectly believed that the data is required for the model to function.

Being clear with the basics is very important, especially if we wish to dive deep into LLMs. There were some misconceptions, however I am thankful that I got the opportunity to learn from my mistakes, and feel much more clear about this.